<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Stack Overflow Analysis on CSS Final Project Group 21</title><link>https://s204151.github.io/Some_page_test/</link><description>Recent content in Stack Overflow Analysis on CSS Final Project Group 21</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://s204151.github.io/Some_page_test/index.xml" rel="self" type="application/rss+xml"/><item><title>Data description</title><link>https://s204151.github.io/Some_page_test/data-description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://s204151.github.io/Some_page_test/data-description/</guid><description>The dataset that has been used for this project is named StackSample. It contains 10% of stack overflows questions, answers and tags data.
The datasets are found on the website: https://www.kaggle.com/datasets/stackoverflow/stacksample
All the datasets used in this project can be downloaded from here: https://drive.google.com/drive/folders/1RdU0mMRiBL9uTIWgPd8ZTfrs3qKUc8qB?usp=sharing
The files that can be downloaded from the website are:
Questions.csv - 1.79 Gb Answers.csv - 1.5 Gb Tags.csv - 65 Mb There is only need for the Questions and Answers files for this project.</description></item><item><title>Network analysis</title><link>https://s204151.github.io/Some_page_test/network-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://s204151.github.io/Some_page_test/network-analysis/</guid><description>To get more information about the users i.e. nodes, the StackOverFlow API is used. Traffic is limited to 300 requests รก day and information from up to 100 users can be retrieved per request.
It is noted that a substantial amount of accounts no longer exist. For example, the user 100027 exists, while 100157 no longer exists. This means that out of the 30133 nodes in our network, only additional information pertaining to 17327 of them could be retrieved.</description></item><item><title>Text analysis</title><link>https://s204151.github.io/Some_page_test/text-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://s204151.github.io/Some_page_test/text-analysis/</guid><description>Throughout the dataset, question body text, question title text and answer body text is tokenized and stopwords are removed. Bi-grams are also found for semi-contextual visualization of discussed topics in different communities using WordClouds.
To measure importance of terms, TF-IDF is used. The (non-normalized) Term Frequency is used,
$$ \text{TF}(t,d) = f_{t,d} $$
Where f_{t,d} is simply the term count in document. The Inverse Document Frequency is calculated as
$$ \text{IDF}(t,D) = log(\frac{N}{1 + n_t}) + 1 $$</description></item></channel></rss>